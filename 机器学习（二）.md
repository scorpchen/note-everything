课程回顾：
1. SVM是最大化间隔（Margin）的分类算法
2. 优化问题：
    训练样本`{（xi， yi）}`， i从1到n，xi是向量，yi是标签
3. 最小化：`0.5||w||^2`, 限制条件：`yi*[w'xi+b]>=1`

# 支持向量机-非线性可分
## 最小化：`0.5||w||^2 + C*sum（ξi）`
其中si为松弛变量（Slack Variable）；`C*sum（ξi）`叫做正则项（Regulation Term）；`C`是实现设定的参数；
限制条件：
- `yi[w'xi + b] >= 1-ξi`
- `ξi >= 0`
## 高维映射φ（x）: x --φ--> φ(x)
- 异或问题：class1：[0, 0],[1, 1];  class2：[1, 0], [0, 1];
    * 定义：`x=[a, b]` --φ--> `φ(x)=[a^2, b^2, a, b, ab]`
    * w = [-1, -1, -1, -1, 6], b=1, 则可以将其分开; 注意到高维空间后，w的维度也升高
- φ（x）使用无限维，我们可以不知道无限维映射φ(x)的显示表达，我们只要知道一个核函数（kernel function）：`K(x1, x2) = φ(x1)'φ(x2)`
    - 这个优化式仍然可解
    - 核函数：
        1. `K(x1, x2) = e^( -(||x1-x2||^2) / (2*σ^2) ) = φ(x1)'φ(x2)`，高斯核
        2. `K(x1, x2) = （X1'*X2 + 1)^d = φ(x1)'φ(x2)`，d为多项式阶数
    - `K(x1, x2)`能写成`φ(x1)'φ(x2)`的充要条件（Mercer’s Theorem）：
        1. `K(x1, x2) = K(x2, x1)`，交换性
        2. 对任意的ci，Xi，有：`SUM( SUM(ci*cj*K(Xi, Xj)) ) >= 0`，半正定性
    - 我们要在只知道K， 不知道φ的情况下解决优化问题：
        * 最小化：`0.5||w||^2 + C*sum（ξi）`
        * 限制条件：
            - `yi[w'φi + b] >= 1-ξi`
            - `ξi >= 0`
## 优化理论：
- 书籍：
    * *Convex optimization*, Stephen Boyol，中文是凸优化
    * *Nonlinear Programming*
- 原问题（Prime Problem）——非常普适
    * 最小化：`f(ω)`
    * 限制条件：`gi(ω) <= 0`; `hi(ω) = 0`;
- 对偶问题（Dual Problem）
    * 定义：`L(ω, α, β) = f(ω) + SUM( αi*gi(ω) ) + SUM( βi*hi(ω) ) = f(ω) + α'*g(ω) + β'*h(ω)`
    * 对偶问题的定义：
        - 最大化：`θ(α, β) = Min(所有ω）{ L(ω, α, β) }`
        - 限制条件：`αi >= 0`
- 定理：如果`ω*`是原问题的解，而`α*, β*`是对偶问题的解，则有：`f(ω*) >= θ(α*, β*)`
    * 证明：`θ(α*, β*) = Min(所有ω）{ L(ω, α*, β*) }<= L(ω*, α*, β*) = f(ω*) + SUM( α*_i * g_i(ω*) ) + SUM( β*_i * h_i(ω*) ) <= f(ω*)`
- 定义：`G = f(ω*) - θ(α*, β*) >= 0`，G叫做原问题与对偶问题的间距（Duality Gap），对于某些特定的优化问题，可以证明`G=0`
- 强对偶定理：若`f(ω)`为凸函数, 且`g(ω) = Aω + b`, `h(ω) = Cω + d`，则此优化问题的原问题与对偶问题的间距为0，即：`f(ω*) = θ(α*, β*)`，此时对于任意i，或者`α*_i = 0`，或者`g_i(ω*) = 0`，（KKT条件）