## 绪论
* Three categories of machine learning algorithm:
    - 有目标的学习：
        1. Supevised learning (有监督的学习) - SVM, NEURAL, NETWORKS
        2. Usupervised learning (无监督的学习) - CLUSTERING, EMAL, PCA
        3. Semi-Supervised Learning (有限监督的学习)
    - 无目标地学习：
        4. Reinforcement learning （增强学习）
* Supervised learning
    - classification   离散标签
    - regression（回归）  连续标签
* 特征提取（重要）——> 机器学习
* 难点：维度，标准
* 没有免费午餐定理（No Free Lunch Thearem):
    - 如果不对特征空间进行先验假设，则所有算法地平均表现是一样的
* 我们认为，特征差距小的样本更可能是同类

## 支持向量机 （Support Vector Machine)
* 发明者：Vapnik（苏联）
* 线性模型（线性可分样本集，Linear Separable; 非线性可分， Non-linear Separable）
    - 定义：训练数据和标签（x1, y1),(x2, y2).......其中，xi为向量，yi为标签（1或-1）；
    - 线性模型：超平面：（w, b) 其中，w为向量，维度同x，且超平面方程 `w'x + b = 0`; 机器学习的任务既是确定w和b
    - 训练集线性可分是指：
        * 存在（w， b），使得对于任意i，
        * 若yi = 1, 则 `w'xi + b >= 0;` 
        * 若yi = -1, 则 `w'xi + b < 0;` 即：`yi * [w'xi + b] >= 0;`
* 优化问题（凸优化问题，二次规划问题）
    - 最小化（Minimize): `0.5 * ||w||^2`
    - 限制条件（Subject to): `yi * [w'xi + b] >= 1;`
    * 事实1：`w'x + b = 0` 与 `aw'x + ab = 0` 是同一个平面，其中， a为实数；
    * 事实2：点到平面的距离公式：`d = |w1x0 + w2y0 + b|/sqrt(w1^2 + w2^2);`
    向量x0到超平面 `w'x + b = 0` 的距离：`d = |w'x0+b|/||w||`
    * 我们可以用a缩放（w, b)使得在支持向量上 `|w'x0+b|= 1` ,此时支持向量与平面的距离为 `d = 1\||w||`
    * 二次规划（Quadratic Pragramming)
        1. 目标函数（Objective Function)是二次项
        2. 限制条件是一次项
        * 要么无解，要么只有一个极值
